---
title: 'R på ucloud'
teaching: 10
exercises: 2
---

:::::::::::::::::::::::::::::::::::::: questions 

- How do you write a lesson using R Markdown and `{sandpaper}`?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Explain how to use markdown with the new lesson template
- Demonstrate how to include pieces of code, figures, and nested challenge blocks

::::::::::::::::::::::::::::::::::::::::::::::::


- making sure that folders are available in the RStudio job 
- setting a subdirectory of "work" as the working directory in RStudio 
- what to do when an Rstudio job seems to freeze (e.g. cannot save file): close the interface window and open it again
- how to download an entire folder, so they can download their work at the end of the semester (without having to learn how to sync folders, which I imagine will be too much to ask from this workshop)
archiver...



All students at UCPH have access to a High Performance Computing (HPC) facility;
ucloud. It can be acessed at cloud.sdu.dk using the normal UCPH login and 
password.


Depending on the user allowances, it will look something like this.

![](fig/ucloud_front.png)

Ucloud provides access to a multitude of different applications organized in the
application store:

![](fig/ucloud_store.png)

Amongst the more popular picks from the store is RStudio:


![](fig/ucloud_picks.png)

This allow us to start a session of RStudio, accessible in the browser. 


![](fig/ucloud_rstudio.png)


Note that we can chose different machine types. Parallel processing is not 
the solution to every problem, but sometimes it is, and here we get access to a
_lot_ of cores.


## This is not a computer as such

What we can start is not a computer. It is a virtual machine. A virtual machine 
is not a real computer, it is a simulation of a computer, that runs on a bigger
computer. Rather than installing an operating system on a real computer, and
install software on it, we ask a larger computer to start a simulation of a 
computer. Since we interact with it through the web, we might not discover the
difference. But it is there!

This allow us to draw on a lot of resources, primarily "cores" and "RAM".

Cores are the physical unit that runs your code. When we ask R to calculate `1+1`,
it is done at the core. And the core can do only one thing at a time. It does it
incredibly fast, but still only one calculation at a time.
Working with a virtual computer, makes it easy to 
get access to more than one core. Having access to more than one core, allow us
to run parallel processes. If we have two set calculations that are independent
of each others, we could send them both to the same core. It would do the first
set of calculations. And when it is done with those, it will do the second set 
of calculations. 
If we have two cores, we could instead send the first set of calculations to 
the first core, and the second set to the second core. And they would do the
calculations independently - we do not have to wait for the first calculation to
finish before we can begin on the second.

Some calculations cannot be done in parallel. And some calculations can be done
in parallel, but will not be done faster. When we choose a machine when we 
start RStudio, we will choose a certain size of machine, and get access to a 
lot of cores.

What we also get access to is more RAM. RAM is the short term memory of the 
computer. If we want to manipulate 10 Gb data, we need to be able to have
10 Gb of data in the memory of the computer. And we probably need to have
access to much more than 10 Gb, because R likes to use a lot of memory, and 
tends to make a copy of the data it manipulates.

We might therefore have to chose a certain size of virtual machine, not because
we actually need 64 cores, but because we need the memory.

Note that the more cores we chose, the more expensive it will get. Ucloud charges
1 core-hour for running 1 core for 1 hour. Running 64 cores for 1 hour will cost
us 64 core-hours.

Do not request more cores than you actually need.

On the other hand, do not request less cores than you need. If you have chosen
too few, you will have to shut down the virtual machine, and start a larger.

And because we are not running on a real computer, everything will be lost.

If you have requested a machine, and have forgotten how much you actually
requested - go to "runs", double click the job running (indicated by a green
symbol to the right), and you will get to a screen telling you that you
"requested 1x u1-standard-2 from DeiC". Go to the place where you ETC TIL MACHINE TYPES
HVOR DU KAN SE AT STANDARD-2 HAR TO CPU'ER OG GIVER DIG 12 GB RAM.

This is also the place where you can stop the virtual machine. By default the 
virtual machine will run for one hour. But if you are finished after 45 minutes,
there is no reason to let it keep running. You are charged by how much time
you actually use. 

It is also here you can extend the runtime. Again - do not extend by 8 hours if
you only need 2. 






If we need to
save our results, we need to save them to files, on our user drive.

If we do not save the results, or for that matter the scripts we write,
all is lost when the virtual machine closes. Either because we close it,
or because the time ran out.

## Setup issues

We can save our work, and as long as we do not delete it again, we have
access to it. And keeping stuff in a project in R makes everything easy(ish) to
locate.

But. Installed libraries in R, are not stored in the project. And when we 
turn off the virtual computer, they disappear. Also for specific uses, we need
to have access to additional software. This is a very common thing to need when
working with machine learning. A lot of the algorithms and models are implemented
in Python, and instead of rewriting all the code to work in R - we can use R
to access the Python solution instead. This is smart. When it works, and
that can be a bit of a problem.

What complicates everything is that we might need a packages called "keras".
But that needs another packaged called "tensorflow". Which requires a package
called "pandas". Which requires a packages called "numpy". And one of them
might only run on a version of Python that is newer than 3.9. And another
one of them might only run on Python older than 3.11.

These complex combinations also needs to be set up for us to be able to do our
work. And they will _also_ disappear when we restart the virtual machine.

Therefore we might need to run a setup script every time we start up the virtual
machine.

Below we present two approaches to this. "The Sledgehammer Solution", which we
have tested, and works, but takes a lot of time to run every single time 
you start a virtual machine. And "The Simpler Solution" which is less complex,
should be faster, usually works, but still have some bugs. 

We hope to add a third approach which we will call "The Good Solution", which
should make it unnessecary to run it every time you start a virtual machine.

### The sledgehammer solution

When you have started the virtual machine, and opened a project, make a new R-script
in the project, copy the following code (or download it from here), and run it
line by line.

```{r eval = F}
# Run shell commands from R
system2("sudo", args = c("add-apt-repository", "-y", "ppa:deadsnakes/ppa"))
system2("sudo", args = c("apt-get", "update"))
system2("sudo", args = c("apt-get", "install", "-y", "python3.9", "python3.9-venv", "python3.9-dev"))

# Python setup
system2("python3.9", args = c("-m", "ensurepip", "--upgrade"))
system2("python3.9", args = c("-m", "pip", "install", "--upgrade", "pip"))
system2("python3.9", args = c("-m", "pip", "install", "numpy"))

# Create and activate environment
system2("python3.9", args = c("-m", "venv", "~/r-tensorflow"))

# Activate virtual environment and install packages
system2("bash", args = c("-c", "source ~/r-tensorflow/bin/activate && pip install numpy tensorflow keras spacy && python -m spacy download en_core_web_sm && deactivate"))

# R packages and setup
Sys.unsetenv('RETICULATE_PYTHON')
library(reticulate)
use_virtualenv('~/r-tensorflow', required = TRUE)

install.packages('remotes')
remotes::install_github('rstudio/tensorflow', upgrade = 'always')
library(tensorflow)
install_tensorflow(envname = '~/r-tensorflow', restart_session = FALSE)

remotes::install_github('rstudio/keras')
library(keras3)
install_keras(envname = '~/r-tensorflow')
```


One way to check that everything worked, is to run the following script:

```{r eval = FALSE}
library(keras3)
model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = c(784)) %>%
  layer_activation("relu") %>%
  layer_dense(units = 10) %>%
  layer_activation("softmax")
summary(model)
```

This should return a model summary. Do not try to find any meaning in it, this is
a simple toy example without any meaning.

One downside to this, is that this takes quite some time, and will have to
be repeated _every_ single time we start the virtual machine.

## The Simpler Solution


Again, make sure that you have made a new project in R-Studio, then make a new
script, and add this code to it. This also need to be run line by line, especially
because we need to restart R a couple of times. 

```{r eval =F}
writeLines('RENV_PATHS_CACHE = "renv/cache"', ".Renviron")
# restart R in order to set the environment variable
.rs.restartR()
renv::init()
library(renv)
renv::install("here", prompt= FALSE)
library(here)

renv::install("reticulate", prompt = FALSE)
renv::install('rstudio/tensorflow', prompt = FALSE)
renv::install("rstudio/keras", prompt = FALSE)

reticulate::install_miniconda(path = "python_env")
.rs.restartR()
reticulate::conda_create(envname = "my_project_env", python_version = "3.10")
reticulate::conda_install(envname = "my_project_env", packages = c("numpy>=1.24"))


reticulate::conda_install(envname = "my_project_env", packages = c("pandas>=2.1"))
reticulate::conda_install(envname = "my_project_env", packages = c("tensorflow>=2.13"))
reticulate::conda_install(envname = "my_project_env", packages = c("keras>=2.13"))
.rs.restartR()
reticulate::use_condaenv("my_project_env", required = TRUE)

renv::snapshot()

# Hent værdierne af miljøvariablerne
python_env_path <- Sys.getenv("RETICULATE_MINICONDA_PYTHON_ENVPATH")
python_fallback <- Sys.getenv("RETICULATE_PYTHON_FALLBACK")
renv_cache <- 'RENV_PATHS_CACHE = renv/cache'


# Opret indhold til filen
content <- paste0(
  "RETICULATE_MINICONDA_PYTHON_ENVPATH = ", python_env_path, "\n",
  "RETICULATE_PYTHON_FALLBACK = ", python_fallback, "\n",
  renv_cache
)

# Skriv til fil
writeLines(content, ".Renviron")
Sys.getenv()

```

Note that this will need to be done for every project you initialize. 
Also note - this takes a looong time...



## notes on this:

Data analysis is not worth much, if we are not able to reproduce our results.
A significant amount of work have therefore gone into providing infrastructure
for exactly that. One issue is the question of which libraries are used for
the analysis.

Enter `renv`. `renv` is a library that establishes scaffolding for installing
libraries in a specific location in an R-project, making it self contained and
easy to distribute. Normally we would distribute a "lock file" that describes 
exactly which versions of which packages are used in a project. 


You will see a lot of stuff in the "files" tab. A folder called "renv", a file
"renv.lock", and probably a file ".Rprofile".

Looking into that, we will find a line of code "source("renv/activate.R")"

When ever we start the project, what ever we have written to 
.Rprofile will be run. What will be run in this case is the script "activate.R"
which does a lot of interesting stuff. The important thing is, that 
the renv-library is started. And whenever we now install a package, it 
will be installed in the renv folder. Do not delve too deep into that, leave it
to the machine.

One issue with this is, that there are still installed packages weird places 
on the machine. Caches of the packages are stored outside our project. The idea
is that other projects might use these cached packages, and cut down on install
time. 

In our case, that is not helpful. This cache will disappear when the virtual
machine is stopped.

In order to handle this, we can specify where the cache should be stored.
We can do that manually. Or, and this is the preffered solution, make a file
.Renviron where we specify where renv should place the cache. Having done that
we need to restart R, and now we can install packages to our hearts delight,
and renv will place both the libraries and the cache in our local project.

An example of a script that sets the environemt file, and installs a selection
of usefull pacakges can be found below. Note that this takes a very long time.
The alternative to this taking a very long time once, is for it to take a very
long time every time we open our project. 

## An even better solution

Will hopefully show up here...


::::::::::::::::::::::::::::::::::::: keypoints 

- Ucloud provide access to a multitude of advanced software that we are not able to run locally
- Running RStudio on Ucloud give us access to more compute and memory than we have on our own computer
- More advanced work might require a quite complicated setup
- Restarting a virtual machine means all our work and setup might be lost, unless we take certain precautions.

::::::::::::::::::::::::::::::::::::::::::::::::

